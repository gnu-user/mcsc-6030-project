\documentclass[oneside]{article}
\usepackage[utf8]{inputenc}

\title{Enhancement of Matrix Multiplication using Parallelization and Evolutionary Strategies \\ \vspace{2 mm} {\Large Final Report}}
\author{Jonathan Gillett}
\date{March 2015}

\usepackage{natbib}
\usepackage{graphicx}
\usepackage{url}
\usepackage{setspace}
\usepackage[ruled,vlined]{algorithm2e}

\begin{document}

\maketitle



\section{Abstract}

\doublespacing
Matrix multiplication is among one of the most widely used and extensively studied operations in computing\cite{raz2002complexity}. The complexity of the naive implementation is $O(N^{3})$\cite{raz2002complexity}, with the theoretical lower bound of this computation shown to be $\Omega(N^{2})$\cite{raz2002complexity}. Despite the difference between the upper and lower bounds, the naive implementation remains the most widely used due to the optimization of cache hits\cite{note2002reducing}; much research has been invested in finding more optimal methods of performing this operation.

Advances in optimizing matrix multiplication, such as the Strassen and Coppersmith–Winograd algorithms\cite{huss1996implementation, coppersmith1987matrix} have focused on limiting the number of expensive multiplication operations performed in order to reduce the computational complexity. Despite the theoretical lower complexity of these algorithms, the performance benefits are often negated by the practical limitations of the implementations for real-world datasets and extremely large constants\cite{robinson2005toward}.

Rather than focusing on theoretical optimizations, recent trends have focused on using approximate methods and recent advances in parallelism based on MapReduce, which is provided by the Hadoop framework\footnote{Hadoop is a distributed file system and parallel computation framework\cite{shvachko2010hadoop} often used for performing MapReduce operations based on specifications published by Google researchers\cite{dean2008mapreduce}}. Large technology companies such as Twitter and Facebook have realized the performance benefits of approximate parallel matrix multiplication operations to perform data analysis and machine learning to maximize ad revenue. Twitter, in particular created an approximate matrix multiplication operation known as DIMSUM, to perform parallel approximate matrix multiplication using MapReduce on matrices hundreds of terabytes in size\cite{zadeh2013dimension}.

Given the recent trend towards approximate parallel computation for matrix multiplication, the emphasis of the research will be to focus on implementing an approximate solution for matrix multiplication. Time permitting, the research will optimize the process further using parallelization and evolutionary strategies to optimize parameters.

For the preliminary results we have implemented a test framework in order to consistently test and validate the different implementations. Currently there are three benchmark implementations that have been created: a naive solution demonstrating the poor performance of iteration in Python; a baseline solution, which uses the \emph{np.dot} function provided by the NumPy library; and finally, a simple parallel implementation using MPI, where the rows are divided amongst N processors.

The desired outcome for the final report is that additional parallel implementations will be created, and an approximate implementation based directly on the work of Drineas and Kannan\cite{drineas2001fast}. The challenge in implementing the approximate solution is to comprehend the theorems and lemmas and to understand their methodology, such that it can be translated into Python and the results reproduced. Lastly, the final goal for the research, time permitting, would be to incorporate parallelism and evolutionary strategies into the approximate implementation.



\section{Introduction}

Matrix multiplication is fundamental to computing, and is an operation widely studied and used in numerical analysis with many real-world applications. The naive implementation has a complexity of $O(N^{3})$\cite{raz2002complexity} with a theoretical lower bound shown to be $\Omega(N^{2})$\cite{raz2002complexity}. For very large matrices this difference can have a major impact in the amount of time required for computations.

However, even though the complexity of the naive implementation is significantly higher than the lower bound, it is still widely used due to highly optimized implementations written in Fortran, that make use of optimal cache hits\cite{note2002reducing}. The performance of the naive algorithm and other linear algebraic operations is so critical that software such as LAPACK (Linear Algebra PACKage)\cite{lapackweb} and ATLAS (Automatically Tuned Linear Algebra Software)\cite{whaley2001automated} have been created to provide optimal implementations of these operations. LAPACK provides many common operations for linear algebra such as matrix multiplication, and has been invaluable to the progress of both academia and industry. Furthermore, LAPACK is also fundamental to the success of MATLAB\cite{matlab2000}, which is one of the most widely used software applications in science and engineering.

In addition to optimizing the implementations of the naive method of matrix multiplication, there have been many recent advances in optimizing the theoretical complexity of matrix multiplication starting with the Strassen and Coppersmith–Winograd algorithms\cite{huss1996implementation, coppersmith1987matrix}. These algorithms have focused on limiting the number of expensive multiplication operations performed in order to reduce the computational complexity. In Strassen's algorithm the number of multiplication operations required is reduced, giving a slightly lower complexity of $O(N^{2.8074})$\cite{huss1996implementation}. In the case of the Coppersmith–Winograd algorithm, which is similar to Strassen's, the complexity is $O(N^{2.374})$\cite{stothers2010complexity}. Nevertheless, despite the theoretical lower complexity of these algorithms, their use of recursion negatively impacts cache access, and the theoretical performance benefits are negated by the more optimal naive implementations, which make use of optimal cache hits\cite{note2002reducing}. In particular, the large constants of the Coppersmith-Winograd algorithm make it only feasible for extremely large matrices, beyond the storage and memory capabilities of present day hardware\cite{robinson2005toward}.

As a result of the practical limitations of the theoretically optimal matrix multiplication algorithms, research and industry have been swift to focus on optimizations of matrix multiplication that sacrifice exact for approximate solutions and make use of highly parallel algorithms. The work by Drineas and Kannan in their seminal paper, \emph{Fast Monte-Carlo Algorithms for Approximate Matrix Multiplication}\cite{drineas2001fast}, and their subsequent three part publication in SIAM\cite{drineas2006fastI, drineas2006fastII, drineas2006fastIII}, laid the groundwork for much of the modern-day approaches to performing approximate linear algebraic operations. Building on the body of research conducted with Drineas and Kannan, Mahoney continued to pursue further applications of approximate stochastic algorithms to algebraic operations. In 2011 Mahoney published a detailed review paper on the subject, \emph{Randomized Algorithms for Matrices and Data}\cite{mahoney2011randomized}, which became highly influential to industry with it's numerous practical applications to machine learning.

The significance of this research caught the attention of Twitter and Facebook, which are data driven organizations that rely on linear algebraic operations for machine learning and data analysis. Twitter created an approximate matrix multiplication operation known as DIMSUM. DIMSUM was used to perform parallel approximate matrix multiplication with MapReduce on matrices hundreds of terabytes in size, and resulted in significant performance improvements\cite{zadeh2013dimension}.


\begin{figure}[ht!]
\centering
\includegraphics[width=\textwidth]{figures/twitter-dimsum}
\caption{DIMSUM, used in production at Twitter since June 2014, has resulted in an observed 40\% performance improvement\cite{zadeh2013dimension}}
\label{fig:transaction}
\end{figure}


Given the importance of optimizing matrix multiplication and the recent emphasis on utilizing approximate stochastic methods and parallelism. Studying these topics is an interesting problem that is relevant to the course given the emphasis on numerical precision and parallelism in High Performance Computing (HPC). Therefore, the purpose of this project will be to focus on the common serial implementations of matrix multiplication and to study the performance benefits of parallel and approximate implementations. Matrix multiplication is a very well defined problem, where the results can be easily verified and benchmarked using the tools and techniques demonstrated in class with Python, Numpy, and MPI\cite{van2011numpy}.

The greatest challenge will be implementing the approximate solutions, which involves reading through papers by Drineas, Kannan, and Mahoney in order to comprehend the theorems and lemmas and to understand their methodology and into Python. Following this, the next challenge will be to implement parallel algorithms for matrix multiplication, for example Cannon's algorithm. Considering that Python is inherently a single process/single threaded language due to the Global Interpreter Lock (GIL)\cite{beazley2010understanding}, parallelism will be implemented using MPI, following the techniques and examples demonstrated in lectures.




\section{Problem Formulation}

Matrix multiplication is a linear algebraic operation that is a fundamental step in many operations in computing. Similarly to the importance of multiplication to arithmetic, matrix multiplication is critical to performing operations on systems of equations and other models for computation. The naive method for matrix multiplication, as shown in algorithm \ref{alg:naive}, involves three nested iterations and intuitively has a computational complexity of $O(N^{3})$.

\begin{algorithm}[!ht]
  \SetKwProg{Fn}{}{}{end}
  \SetKwFunction{Naive}{Naive}
  \SetKwArray{Forwards}{forwards}
  \SetKwFunction{ZeroMatrix}{zeroMatrix}
  \SetAlgoNoLine
  \DontPrintSemicolon
  
  \Fn{\Naive{$A, B$}}{
  \tcp{C is a zero matrix with dimensions from A, B}
  $C \leftarrow $ \ZeroMatrix($A_{rows}, B_{cols}$)
  \BlankLine
  
  \For{$i \leftarrow 1$ \KwTo $A_{rows}$}
  {
    \For{$j \leftarrow 1$ \KwTo $B_{cols}$}
    {
      \For{$k \leftarrow 1$ \KwTo $A_{cols}$}
      {
        $C_{i, j} \leftarrow C_{i, j} + A_{i, k} * B_{k, j}$
      }
    }
  }
  \BlankLine
  \KwRet($C$)
}
\caption{Naive Matrix Multiplication}
\label{alg:naive}
\end{algorithm}


The problem at hand involves studying the optimizations of the naive matrix multiplication and utilizing approximate and parallel solutions based on existing literature, and research to improve it. Furthermore, in order to consistently measure the performance improvements of the approximate and parallel implementations, a test framework must be created. The test framework will be used to execute each of the implementations against a variety of test matrices and to perform numerous trials in order to record accurate statistical results. The results of each implementation will then be compared to the performance results of the naive implementation provided by the \emph{np.dot} function, which is part of the NumPy library.




\subsection{Solution Methodology}

In order to complete this project the solution methodology will consist of selecting suitable matrices of different sizes so that the performance benchmarks are consistent and accurate. Following this, a framework for testing the implementations will be produced to automate testing each implementation against a set of matrices determined for the experiments. Lastly, each of the proposed approaches will be evaluated and each implementation will be benchmarked and compared to the baseline result of the serial naive implementation.

The following is the revised solution methodology, based on the initial project proposal.

\begin{enumerate}
\item For the purpose of benchmarking, a set of suitable matrices will be defined, in particular large matrices representing graphs and stochastic models will be used based on their numerous applications\cite{yegnanarayanan2013application}.

\item The results for the approximate algorithms will be compared to the exact results of the \emph{np.dot} operation using the norm of the error matrix, which results from the subtraction of the approximate matrix, $P$, from the exact result, $P - A \cdot B$\cite{drineas2001fast}. Furthermore, for the test matrices representing stochastic models, the steady-state eigenvalues will also be compared and recorded during the benchmarking.

\item A Python test framework will be created that will execute all of the tests and record the execution times and the result of multiplication calculations. This will be used to automate the benchmarking process and consistently and accurately benchmark the parallel and approximate implementations and evaluate the results.

\item The serial baseline benchmark will be implemented using the \emph{np.dot} operation and tested using the framework as a base reference to compare against each of the approximate and parallel implementations.

\item Parallel algorithms such as Cannon's algorithm\cite{lee1997generalized}, will be implemented using MPI and Numpy and tested using the framework. The results will be compared to the performance of the baseline implementation.

\item Approximate algorithms will be implemented directly based on the paper, \emph{Fast Monte-Carlo Algorithms for Approximate Matrix Multiplication}\cite{drineas2001fast}, by Drineas and Kannan.

\item Time permitting, evolutionary strategies such as genetic algorithms and differential evolution, which are often used in optimization\cite{de1989using}, will be used to optimize parameters for the approximate algorithms.

\item All implementations will be compared against each other and the reference baseline using the framework and the performance results and the margin of error, for approximate implementations, will be recorded.
\end{enumerate}




\section{Preliminary Results}

The preliminary results so far have been the creation of several implementations that can be used to compare the performance of matrix multiplication, including the baseline implementation, a slower naive implementation demonstrating the poor performance of iteration in Python, and lastly a simple parallel implementation using MPI.

In addition to creating several implementations, a test framework has also been created to automate the process of benchmarking implementations and comparing the results. The test framework parses a test plan document, which is written using YAML, that describes each of the benchmark codes, the tests, the matrices used for the tests, and the order in which tests are executed. The following snippet shows the versatility of the test plan YAML document that is parsed by the test framework.


\singlespacing
\begin{verbatim}
benchmarks:
    - name: simple_parallel
      description: Simple parallel implementation using MPI
      file: simple_parallel.py
      exec: mpiexec
      args: ['-np', '4', 'python2']
tests:
    - name: assorted_float
      description: Tests with uniform floats of [0, 1).
      dimensions: [25, 50, 75]
      type: float
testplan:
    - test: assorted_float
      trials: 10
\end{verbatim}


\doublespacing
The creation of the test framework greatly simplifies the process of executing the benchmarks and validating the performance results by automating the entire process. All that is required for the test framework are the benchmarks and a test plan. The test plan outlines the test cases to execute; the test matrices to generate; the codes to execute; and the order in which tests are to be executed. The test framework then parses this test plan and executes it displaying the progress of each test as shown in figure \ref{fig:progress}. After the test plan is completed summary statistics are displayed. As shown in figure \ref{fig:summary} a table displays the total average execution time for each of the benchmarks and their respective test results. In this example the \emph{simple\_parallel} implementation is marginally slower than the baseline.

\begin{figure}[ht!]
\centering
\includegraphics[width=\textwidth]{figures/framework-progress}
\caption{Test framework displaying test progress}
\label{fig:progress}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=\textwidth]{figures/framework-summary}
\caption{Test framework displaying summary statistics}
\label{fig:summary}
\end{figure}



\subsection{Expected Outcomes}

The following is the list of expected outcomes, which as been refined from the initial project proposal based on the preliminary results.

\begin{enumerate}
\item Support for generating large adjacency matrices representing graphs and stochastic matrices representing datasets that are similar to real world applications.

\item Refinement of the test framework to support evaluating approximate matrix multiplication implementations and evaluating the resultant matrix using the norm of the error matrix. As well, in the case of stochastic matrices by comparing the steady-state eigenvalues.

\item More parallel implementations, in particular an implementation based on Cannon's algorithm, which is one of the most widely used methods for parallel matrix multiplication.

\item Approximate implementations directly based on the paper, \emph{Fast Monte-Carlo Algorithms for Approximate Matrix Multiplication}\cite{drineas2001fast}, by Drineas and Kannan.

\item A final report discussing the approximate and parallel implementations, their practical applications to large adjacency and stochastic matrices, in addition to a thorough analysis of the performance results in comparison to the baseline implementation.

\end{enumerate}




\section{Reflections}

%- State clearly how you fared in your project with respect to the goals that you set out to achieve. 
%- In particular, note those original goals that you did achieve and explain their significance.

All of the main goals specified in the \emph{solution methodology} and the expected outcomes of the preliminary progress report were met with a few minor exceptions. In particular, a suitable set of matrices were defined in order to ensure that the benchmarking would be accurate, including matrices representing graphs and stochastic processes; refinement of the test framework to support evaluating approximate matrix multiplication; an approximate matrix multiplication implementation based on the paper, \emph{Fast Monte-Carlo Algorithms for Approximate Matrix Multiplication}\cite{drineas2001fast}, by Drineas and Kannan; and lastly a set of consistent benchmarking results to discuss the performance results of the various implementations.

While all of the main goals and expected outcomes were met there were a few minor exceptions, the first being the lack of a parallel implementation based on Cannon's algorithm. This was due to the limited performance benefit, as Cannon's algorithm merely reduces the bandwidth overhead\cite{lee1997generalized} and limited remaining time as a result of challenges implementing the approximate matrix implementation as discussed in the following section. The second exception being the lack of a consistent methodology to evaluate the accuracy of the approximate matrix multiplication results, while the algorithms were implemented as accurately as possible based on literature, the error bounds calculations could not be implemented in a way that was reliable for testing.



\subsection{Challenges \& Successes}

% - Also, make note of those goals that may not have turned out the way you had originally anticipated.
% - Document any inconsistencies you may have found in the other literature that you have read while compiling your report
% - Give reasons for this based on your experiences from working on your project.

While the intended goal of writing an implementation in Python based on the approximate matrix multiplication concept published by Drineas and Kannan was met there were numerous issues in understanding how to correctly implement the algorithm. The paper by Drineas and Kannan focused on the proofs for the error bounds and reducing the error bounds using non-uniform probabilities there was little information on how to properly implement the discussed algorithms with scarce information on how the probabilities are determined, and how to perform the scaling. Furthermore, for practical purposes it appears that the error bounds only decrease once the matrix is extremely large, such as was demonstrated by Drineas and Kannan\cite{drineas2001fast}.

Exasperating the problem of properly implementing the approximate matrix multiplication algorithm were inconsistent notation and explanations from other papers. After numerous issues implementing the approximate matrix multiplication algorithm there was a breakthrough after discovering a set of lecture slides written by Michael Mahoney, the third author on the SIAM publication \emph{Fast Monte Carlo algorithms for matrices I: Approximating matrix multiplication}\cite{drineas2006fastI}. Between the two sets of lecture slides\cite{mahoneyCS369M, mahoneyCS294}, Mahoney provided the algorithms used to perform the calculation of the non-uniform probabilities in addition to the approximate matrix scaling and multiplication\footnote{These lecture slides were part of a course \emph{Algorithms for Massive Data Set Analysis} taught by Mahoney at Stanford.}


\begin{algorithm}[!ht]
  \SetKwProg{Fn}{}{}{end}
  \SetKwFunction{Approx}{Approx}
  \SetKwArray{Forwards}{forwards}
  \SetKwFunction{ZeroMatrix}{zeroMatrix}
  \SetKwInOut{Input}{input}\SetKwInOut{Output}{output}
  \SetAlgoNoLine
  \DontPrintSemicolon
  \Fn{\Approx{$A, B$}}{
  \Input{$A \epsilon R^{m \times{} n}, B \epsilon R^{n \times{} p}, C \epsilon Z^{+}$ s.t. $1 \leq c \leq n$ and $(p_i)_{i=1}^n$ are s.t. $p_i \geq 0$ and $\sum_{i=1}^{n} p_i = 1$}
  \Output{$C, R$}
  \BlankLine
  
  \For{$t \leftarrow 1$ \KwTo $c$}
  {
    Pick $i_{t} \epsilon 1,...,n$ with $Pr[i_{t} = k] = p_{k}, k = 1,...,n$ independently
    
    Set $C^{(t)} = A^{(i_t)} / \sqrt{cp_{i_t}}$
    
    Set $R_{(t)} = B_{(i_t)} / \sqrt{cp_{i_t}}$
  }
  \BlankLine
  \KwRet($C, R$)
}
\caption{Approximate Matrix Multiplication}
\label{alg:approx}
\end{algorithm}

Algorithm \ref{alg:approx} demonstrates the approximate matrix multiplication method defined by Mahoney in the set of lecture slides for the courses taught at Stanford\cite{mahoneyCS369M, mahoneyCS294}. With the algorithm provided it was possible to implement the approximate matrix multiplication algorithm in Python, a major hurdle in accomplishing all of the desired goals. Understanding the approximate matrix multiplication method proved to be the greatest challenge of the project and required an incredibly detailed literature review of all of the authors of the original papers and their subsequent publications. Even so, it was not until a set of lecture slides for an undergraduate course taught by Mahoney at Stanford was discovered before the algorithm was completely understood. 







\section{Final Results}

The test framework was expanded to add support for executing the approximate matrix multiplication benchmarks in addition to displaying the execution time for the results of each test. The test plan was updated to include new tests using more realistic data with adjacency matrices, which record the vertices and edges of a graph, and stochastic matrices which represent a stochastic system such as those represented by a Markov chain\cite{yegnanarayanan2013application}.

In order to ensure as accurate results as possible for the experiments the test environment was controlled as much as possible to limit external sources of error. The test platform used was a Lenovo i7 Thinkpad, with four cores (with hyper-threading) at 2.8 GHz, due to limitations and inconsistencies using SHARCNET. All processes were terminated in order to limit any external influence on performance results and the benchmark environment was executed on 10 separate instances, each execution with 10 trials performed for each benchmark tested, to ensure accurate results and a large statistical sample.

- discuss test framework from previous sections

- discuss how the testplan was updated to include new tests using more realistic data with adjacency matrix, which represents the vertexes and edges of a graph, and stochastic matrices which represent a stochastical
system such as those represented by a markov chain CITE(APP. PAPER)

- discuss the number of trials performed for each test to ensure accurate results
- the test platform used (lenovo i7 thinkpad) due to limitations/issues with SHARCNET environment, all processes were terminated in order to limit any external influence on performance results



5. Include a table with the results of the "numerous" trials executed for each of the sizes tested

    --> Mention that for smaller tests the results of approx. random selection method is not as good (CITE big-O complexity) due to the difficulty in vectorizing the operations using a mask as it does random selection WITH replacement and order of selected columns also is important


6. Include 2 plots of different tests

- Discuss how results show that the performance is better for larger dimensions for the approximate implementation in comparison to even the parallel due to the reduction of any networking overhead, and
the reduction in size of matrix for copmutation due of $O(N^{3})$ and how small reductions in size can have
a large performance impact for large matrices

- CIte how true results could be better realized with extremely large matrices, cite how the work of drineas and kannan involves matrices over 10,000 x 10,000 in size and the matrices used for the operations performed by twitter's approx. solution uses matrices containing terabytes of data

- Parallelization of approx. matrix multiplicatoin is difficult due to the inherently sequential nature of the pass used to calculate the statisitcs for the non-uniform sampling, as mention in paper by drin and kan. after the appropriate rows and corresponding columns have been selected to perform the multiplication any underlying
matrix mult. implementation can be used, including those that make use of parallelization



\begin{table}[ht!]
\centering
\caption{Benchmark Execution Time Results}
{\renewcommand{\arraystretch}{1.15}}
\begin{tabular}{|l|c|l|l|l|l|l|} \hline
benchmarks&        dims&       bool&        int&       float&    adjacency&    stochastic\\ \hline
baseline&             25&      0.000&       0.000&     0.000&    0.000&     0.000\\ \hline
simple\_parallel&     25&      0.003&       0.002&     0.003&    0.002&     0.002\\ \hline
approx&               25&      0.000&       0.000&     0.000&    0.000&     0.000\\ \hline
baseline&             50&      0.000&       0.000&     0.000&    0.000&     0.000\\ \hline
simple\_parallel&     50&      0.004&       0.004&     0.004&    0.004&     0.003\\ \hline
approx&               50&      0.000&       0.000&     0.001&    0.001&     0.000\\ \hline
baseline&            100&      0.001&       0.001&     0.001&    0.001&     0.001\\ \hline
simple\_parallel&    100&      0.006&       0.008&     0.007&    0.005&     0.004\\ \hline
approx&              100&      0.002&       0.002&     0.002&    0.002&     0.002\\ \hline
baseline&            500&      0.142&       0.193&     0.289&    0.187&     0.250\\ \hline
simple\_parallel&    500&      0.161&       0.252&     0.498&    0.218&     0.459\\ \hline
approx&              500&      0.152&       0.175&     0.179&    0.143&     0.147\\ \hline
baseline&           1000&      1.539&       2.827&     2.611&    2.607&     2.342\\ \hline
simple\_parallel&   1000&      1.385&       3.143&     2.332&    2.373&     1.764\\ \hline
approx&             1000&      1.908&       3.276&     1.843&    1.773&     1.778\\ \hline
\end{tabular}
\label{table:benchmark}
\end{table}



\begin{figure}[ht!]
\centering
\includegraphics[width=\textwidth]{figures/assorted_float}
\caption{Assorted Float Test Execution Time}
\label{fig:assorted_float}
\end{figure}

\begin{figure}[ht!]
\centering
\includegraphics[width=\textwidth]{figures/stochastic}
\caption{Stochastic Test Execution Time}
\label{fig:stochastic}
\end{figure}



\section{Concluding Remarks}


x. Conclude with the remark on how the work of drineas and kannan has been expanded by Twitter and Facebook (CITE their papers) and how there are numerous applications of approx. matrix multiplication especially
in use for machine learning



\bibliographystyle{plain}
\singlespacing
\bibliography{references}
\end{document}
